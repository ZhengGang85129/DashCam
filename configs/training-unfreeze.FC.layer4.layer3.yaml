stage_name: unfreezing.FC.layer4.layer3  #single
batch_size: 10
epochs: 5
decay_coefficient: 30
resume: False # If True, the model will load parameters from `check_point_path`
optim_resume: False # If True, the optimizer will load saved state from `optim_check_point_path` 
optimizer:
  name: adamw
  lr_groups: 
    - layer_name: "layer3"
      lr: 1e-5
      weight_decay: 1e-4
    - layer_name: "layer4"
      lr: 1e-5
      weight_decay: 1e-5
    - layer_name: "fc"
      lr: 1e-5
      weight_decay: 1e-5
scheduler:
  name: CosineAnnealingLR #options: 
  T_max: 10
  eta_min: 1e-6
  #name: ReduceLROnPlateau
  #mode: min
  #factor: 0.5
  #patience: 3
  #min_lr: 1e-6
model_dir: stage2 # will store the trained checkpoint under this folder
monitor_dir: stage2-monitor # will store the metric points under this. Currently, this will be overwritten by a new training process.
trainable_parts: ["fc", "layer4", "layer3"] # options: ["*"], ["classifier"], ["layer4", "classifier"]
check_point_path: #valid only if resume is True
optim_check_point_path: #valid only if optim_resume is True