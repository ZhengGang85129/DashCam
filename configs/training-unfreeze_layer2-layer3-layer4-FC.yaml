strategy: single #single
batch_size: 10
resume: True # If True, the model will load parameters from `check_point_path`
optim_resume: False # If True, the optimizer will load saved state from `optim_check_point_path` 
optimizer:
  name: adamw
  lr: 1e-6
  weight_decay: 1e-4
scheduler:
  name: CosineAnnealingLR #options: 
  T_max: 10
  eta_min: 1e-8
  #name: ReduceLROnPlateau
  #mode: min
  #factor: 0.5
  #patience: 3
  #min_lr: 1e-6
epochs: 30
model_dir: uniform_sampling-unfreeze_layer2_layer3_layer4_FC # will store the trained checkpoint under this folder
monitor_dir: uniform_sampling-unfreeze_layer2_layer3_layer4_FC-monitor # will store the metric points under this. Currently, this will be overwritten by a new training process.
trainable_parts: ["layer2", "layer3", "layer4", "classifier"] # options: ["*"], ["classifier"], ["layer4", "classifier"]
check_point_path: benmark_single/model_ckpt-epoch06_bs10_lr1e-5_aug0.25_col_fog_gau_hor_noi_rai.pt #valid only if resume is True
optim_check_point_path: #valid only if optim_resume is True